\chapter{Background and State of the Art}

To contextualize the contributions of this thesis, it is necessary to establish the theoretical foundations upon which it is built. This chapter explores the evolution of distributed systems toward collective intelligence and examines the formalisms of self-organizing frameworks. By evaluating the limitations of current simulators, this chapter identifies the technical `reality gap' that this research aims to bridge, providing the necessary background to appreciate the integration of high-fidelity game engines into the decentralized coordination workflow.

\section{Distributed Systems}

The foundational definition of a distributed system is provided by Andrew S. Tanenbaum and Maarten Van Steen, who describe it as ``a collection of autonomous computing elements that appears to its users as a single coherent system''~\cite{Tanenbaum_Van_Steen_2001b}. This definition establishes a fundamental tension between the independent nature of the underlying hardware and the unified experience required by the end-user. To fully understand the implications of this paradigm, it is necessary to examine these two components individually.

\paragraph{Collection of autonomous computing elements}
The first part of the definition emphasizes the heterogeneity and autonomy of the system's components. These nodes can range from high-performance server clusters to resource-constrained microcontrollers, each operating independently without a shared global clock. Consequently, synchronization becomes a primary challenge, as nodes must rely on message-passing to coordinate their actions toward a common goal.

Furthermore, the nature of these collections is increasingly moving from static, hard-wired topologies toward dynamic environments where nodes join and leave the system, a phenomenon known as \emph{churn}~\cite{stutzbach2006understanding}. In such dynamic settings, maintaining a reliable communication path between any two nodes is not guaranteed and must be managed through robust routing and discovery protocols. This autonomy also introduces security and confidentiality concerns: as nodes act independently, the overhead of authentication and secure message exchange can often become a bottleneck for system-wide scalability.

\paragraph{Single coherent system}
Despite the underlying decentralization, the system must present itself as a unified entity. Coherence implies that the system's behavior remains consistent and predictable from the user's perspective, regardless of where or when the interaction occurs. This requirement leads to the concept of \emph{distribution transparency}, which aims to hide the complexities of the distributed nature of the system from both users and application developers.

A critical aspect of achieving this coherence is the management of failures. In a distributed context, the `perfect network' is a fallacy; instead, systems must be designed under the assumption that components will fail. Treating failures as first-class citizens ensures that the system can maintain its coherent state through fault-tolerance mechanisms, preventing local disruptions from cascading into global system outages.

\paragraph{The Role of Middleware}
To bridge the gap between autonomous nodes and the need for coherence, a \emph{middleware} layer is typically introduced~\cite{Bernstein_1996}. Middleware acts as an abstraction layer that sits between the operating system and the distributed application, providing a standardized interface for resource sharing and communication. By masking the differences in hardware and local software, middleware enables the development of complex distributed logic without requiring the programmer to manage low-level networking details. In the context of this research, \acp{CAS} and \acp{AC} frameworks can be viewed as specialized middleware designed to manage collective intelligence across high-density, pervasive networks.

\subsection{\aclp{DS} Goals}

As cited in~\cref{chap:intro}, the necessity of \ac{DS} arose from the need of coordinating complex, geographically distributed nodes. In particular, the main goals of a \ac{DS} are
%
\begin{inlinelist}
  \item supporting sharing of resources across the network;
  \item hiding the network itself from the user;
  \item be open;
  \item easy scaling.
\end{inlinelist}
%
Building a \ac{DS} is not an easy task therefore these goals should be met in order to making it worth the effort.

\paragraph{Sharing Resources}
The primary motivation for building a distributed system is to facilitate the seamless sharing of resources among a collection of users and applications. In this context, a `resource' is defined broadly, encompassing everything from hardware components like high-capacity storage and specialized sensors to software-defined entities such as data sets and services. From an economic perspective, this allows for the creation of more cost-effective systems by leveraging remote, centralized assets thereby reducing the need for expensive local infrastructure.

Beyond cost efficiency, resource sharing is the fundamental enabler of collaboration in decentralized environments. By allowing autonomous nodes to exchange information and access shared state, the system can tackle complex tasks that would be impossible for a single isolated node to perform.

\paragraph{Distribution Transaprency}
The concept of information hiding is a cornerstone of modern software engineering, allowing developers and users to operate at higher levels of abstraction while delegating low-level complexities to the underlying machinery. In the context of distributed systems, this is realized through \emph{distribution transparency}, which aims to present the collection of autonomous nodes as a single, coherent entity. This transparency is not a monolithic feature but a multi-dimensional goal comprising several distinct types.

\emph{Access} transparency ensures that the way data is represented remains decoupled from the method used to access it, allowing heterogeneous machines to exchange information despite differing internal architectures. \emph{Location} transparency masks the physical coordinates of a resource, much like a \ac{URL} provides a logical address without revealing the specific geographic or network location of a server. Related to this are \emph{migration} and \emph{relocation} transparency, which hide the movement of resources; the former applies to resources moved while not in use, while the latter ensures that a resource can be moved during active use without interrupting the user's operation.

To ensure performance and reliability, \emph{replication} transparency hides the fact that multiple copies of data may exist across the network, while \emph{concurrency} transparency allows multiple users to share a single resource simultaneously without realizing that others are accessing the same data. Finally, \emph{failure} transparency is perhaps the most critical for system resilience, as it attempts to mask the malfunction and subsequent recovery of components, ensuring that the user remains unaware of any underlying faults.

However, achieving full transparency is often more of a strategic trade-off than an absolute requirement. A system that over-prioritizes transparency may inadvertently degrade the user experience. For instance, an aggressive pursuit of failure transparency might lead to a system that hangs indefinitely while repeatedly retrying a connection to a dead server, rather than promptly informing the user of the failure.

\paragraph{Openness}

An open distributed system is characterized by its ability to be easily extended and integrated with other heterogeneous components. According to Tanenbaum and Van Steen, this goal is achieved by ensuring that components adhere to standardized rules governing their syntax and semantics. The practical value of openness is measured through two key metrics: interoperability and portability. Interoperability refers to the extent to which components from different manufacturers can work together based on a common standard, while portability describes the ease with which an application can be moved from one distributed system to another without modification. Furthermore, an open system must be extensible, allowing developers to add or replace internal parts without disrupting the existing infrastructure. This requires moving away from monolithic architectures toward a modular design where internal interfaces are as well-defined as external ones.

A critical design principle for achieving this flexibility is the separation of policy from mechanism. This paradigm suggests that a system should provide a set of general-purpose mechanisms (e.g. the ability to cache data or route messages) while leaving the specific policies (e.g. which data to cache or how long to store it) to the user or application developer. While a strict separation can lead to complex configuration problems, it is essential for creating adaptable systems. In the context of this research, this principle is foundational: the game engine provides the physical and spatial mechanisms, while the Aggregate Computing framework implements the coordination policies that govern the collective's behavior. This balance allows the system to remain open to new environmental constraints or logic changes without requiring a complete redesign of the underlying simulation or communication layers.

\paragraph{Scalability}
Scalability is the ability of a system to handle increasing load without a proportional loss in performance, and it is measured along three dimensions: size, geography, and administration. Size scalability addresses the elimination of centralized bottlenecks (e.g. CPU or bandwidth limits) that occur when a single server is overwhelmed by requests. Geographical scalability deals with the transition from local-area networks to wide-area systems, where synchronous communication becomes impractical due to significant signal latency. Finally, administrative scalability ensures the system remains manageable across multiple trust domains and conflicting security policies.

To achieve these goals, systems typically rely on three core techniques: hiding communication latencies, distribution, and replication. By moving toward asynchronous communication, the system can mask the delays inherent in distant or unreliable links. Furthermore, by distributing work and replicating data across the network, the system avoids single points of failure and bottlenecking.

\subsection{\aclp{DS} Kinds}

To conclude the foundational overview of distributed systems, it is essential to categorize them based on their application focus. While the underlying principles of resource sharing and transparency remain constant, the architectural priorities shift significantly depending on whether the system is designed for raw computation, data management, or environmental integration.

\paragraph{High-Performance Distributed Systems}
Historically, the need for massive computational power led to the development of systems focused on parallel processing. This category includes cluster computing, where a collection of similar workstations is connected via a high-speed local area network to act as a single multi-processor (such as the distributed shared-memory multicomputers~\cite{Amar_Barak_Shiloh_2004}), and grid computing, which federates resources from diverse administrative domains to solve large-scale scientific problems (such as~\cite{Foster_Kesselman_Tuecke_2004}). More recently, cloud computing has evolved these concepts into a utility model, providing scalable, on-demand virtualized resources. In these systems, the primary metric of success is throughput and the efficient distribution of heavy workloads across a stable, high-performance infrastructure.

\paragraph{Distributed Information Systems}
As businesses grew, the challenge shifted from raw calculation to the management of vast amounts of networked data. These systems often evolved from the need to integrate legacy applications into a coherent workflow. Distributed transaction processing ensures that complex operations across multiple databases (such as a bank transfer) remain atomic and consistent, while Enterprise Application Integration~\cite{Enterprise} focuses on allowing decoupled applications to communicate via middleware. Here, the focus is on data integrity and the seamless interoperability of heterogeneous software components.

\paragraph{Pervasive Systems}
The most recent evolution, and the one most pertinent to this research, is the rise of pervasive (or ubiquitous) systems. Unlike the previous categories, which are often composed of stable, `tethered' nodes, pervasive systems are characterized by their integration into the physical environment. These systems are typically composed of small, mobile, and battery-powered devices that must operate with high degrees of instability.

In pervasive environments, the traditional goals of distribution transparency must be re-evaluated. Because these nodes are often context-aware and physically embodied, their `location' is not a detail to be hidden, but a primary input for their logic. Furthermore, the inherent volatility of these networks, where nodes frequently join, leave or fail due to energy constraints, makes centralized management impossible. This shift from static, managed clusters to fluid, unmanaged environments provides the natural transition toward Self-Organizing Frameworks. To maintain a `single coherent system' view in such chaotic settings, we must move beyond simple middleware toward collective adaptive strategies that allow global behavior to emerge from local interactions.

\section{\aclp{CAS}}

\aclp{CAS} represent a sophisticated class of decentralized architectures where global behavior emerges from the nonlinear interactions of numerous autonomous agents~\cite{holland1992adaptation}. Unlike traditional distributed systems that often rely on structured hierarchies or strict consensus protocols, a \ac{CAS} is defined by a large number of spatially situated devices that operate under a set of fundamental constraints. These include predominantly local interactions and a high degree of dynamism. This dynamism manifests as constant fluctuations in the environment and the network topology, often driven by agent mobility, hardware failures, or intermittent connectivity~\cite{DBLP:journals/csur/Casadei23}. Furthermore, these systems typically function in an open setting, an environment where components may appear or disappear over time without destabilizing the system's core utility~\cite{DBLP:journals/jlap/ViroliBDACP19}.

A great challenge in the study of \ac{CAS} is the shift in perspective required to describe behavior. While traditional software engineering focuses on the `single', the desired outcomes in a \ac{CAS} are naturally expressed at the `whole' or collective level. Examples of such outcomes include the estimation of a spatial distribution, the formation of a self-healing geometric pattern, or the adaptive routing of information through a shifting region of space. The core difficulty lies in the gap between this global intent and the local execution; because the system is sensitive to its environment, minor changes in topology, device density, or message timing can lead to qualitatively different emergent outcomes~\cite{BealIEEEComputer2015}. Consequently, the behavior of a \ac{CAS} cannot be predicted by simply summing the individual parts, as the recursive feedback loops between agents create a system-wide complexity that transcends the logic of any single device.

\subsection{Self-Organizing Frameworks}

The engineering of self-organizing systems has led to a variety of frameworks aimed at narrowing the gap between local rules and global emergence. These frameworks generally fall into three categories: bio-inspired coordination models, virtual physics-based approaches, and general-purpose macro-programming languages. Each provides a different lens through which to represent and control the collective state of a \ac{CAS}.

Bio-inspired frameworks often take cues from natural phenomena such as ant colony behavior or chemical gradients. One notable example is the use of \emph{stigmergy}, where agents interact indirectly by modifying their environment (for instance, through digital pheromones that guide other agents toward specific tasks or regions~\cite{DBLP:journals/computer/GardelliVZ08}). Similarly, chemical-inspired models like the \ac{CHAM} treat the system as a solution where floating molecules (data) interact according to fixed reaction rules, allowing global patterns to emerge from purely local, stochastic encounters~\cite{DBLP:journals/tcs/BerryB92}. These models are inherently robust to the arrival and departure of components, as the coordination logic is embedded in the environment rather than in a fixed network map.

Virtual physics-based approaches, such as \ac{APF}, represent the collective by mapping agent interactions to physical forces. In this representation, goals exert attractive forces while obstacles or neighbors exert repulsive forces. The resulting behavior is a continuous movement toward a state of minimum energy, which naturally translates to tasks like collision avoidance or flocking~\cite{DBLP:journals/ijrr/Khatib86}. While highly effective for spatial coordination, these models can struggle with complex logical tasks that require discrete state transitions or symbolic reasoning across the entire network.

A more recent shift toward addressing these limitations is found in formal macro-programming languages. For instance, the \ac{XC} provides a functional foundation where state and interaction are unified into a single primitive, allowing for a more general representation of collective processes that can spread, shrink, or move across a network~\cite{DBLP:journals/fgcs/CasadeiVAP24}. These frameworks move away from specific metaphors (like physics or biology) toward a more general-purpose computational logic.

Among these solutions, \emph{aggregate programming} has emerged as a particularly comprehensive framework for spatially situated collectives. It leverages the abstraction of the \emph{computational field} to represent system-wide data as a continuous entity across space and time. By focusing on field transformations rather than individual message passing, it provides a mathematical and operational bridge to handle the inherent complexity of \acp{CAS}. This approach forms the basis for the subsequent discussion on aggregate computing and its role in modern distributed systems engineering.

\section{Aggregate Computing}

Building on the broad landscape of self-organizing frameworks, aggregate computing offers a rigorous, functional approach to the engineering of spatially situated collectives. It is grounded in the `macro-programming' philosophy, which states that developers should describe the intended behavior of the aggregate as a single entity rather than hacking individual device solutions. This paradigm is centered on a declarative view of collective behavior, where the primary objective is to manage the flow of information across space and time.

\subsection{Field Calculus and Global Semantics}

The mathematical foundation of aggregate computing is the field calculus, a specialized $\lambda$-calculus~\cite{DBLP:journals/tocl/AudritoVDPB19} where every expression evaluates to a computational field. A computational field is the key abstraction of this paradigm; it can be viewed as a `spatial stream' or a space-time structure that maps a domain of events, representing a device firing at a specific time and position, to values~\cite{DBLP:journals/jlap/ViroliBDACP19}. By manipulating these fields through pure functions, developers can compose complex global patterns from simple, reusable building blocks.

The syntax of the field calculus is built around four fundamental constructs that handle value definition, function application, state evolution and neighborhood interaction:
\begin{itemize}
\item Built-in and local operators: these represent standard values (numbers, booleans) and pure mathematical or local sensing operations.
\item Function application: this allows for the functional composition of collective behaviors, where user-defined functions are applied across the field.
\item Repetition (\monospace{rep}): this construct handles the evolution of state over time. It applies a function to the value from the device's previous computational round, enabling the creation of temporal patterns like timers or persistent states.
\item Neighborhood interaction (\monospace{nbr}): this is the mechanism for spatial coordination. It gathers values from the most recent rounds of a device's neighbors, allowing the computation to adapt to the local physical topology~\cite{DBLP:journals/tocl/AudritoVDPB19}.
\end{itemize}

Through these constructs, aggregate computing can express a vast range of spatio-temporal patterns, including the spreading of information (G), the collection and aggregation of data (C) and local state transitions (T)~\cite{DBLP:journals/tomacs/ViroliABDP18}. These `GCT' building blocks form an \ac{API} that enhances raw \monospace{rep} and \monospace{nbr} calls, providing a higher-level language for resilient services like crowd estimation or emergency evacuation alerts.

\subsection{Operational Semantics and the Local Execution Loop}

While the field calculus provides a global, denotational view of the system, its operational semantics define how this behavior is enacted on individual devices. In this model, the `program of the single' consists of repeatedly executing the shared aggregate program in asynchronous rounds. The local execution cycle follows a precise sequence:
\begin{inlinelist}
\item the device reads local sensor values and gathers the state trees (round results) received from its neighbors;
\item the shared aggregate program is evaluated against the device's own previous state and the aligned neighbor trees;
\item this evaluation produces a new local state and a message containing the updated state tree, which is then broadcast to neighbors~\cite{DBLP:journals/jlap/ViroliBDACP19}.
\end{inlinelist}

This local loop ensures that the collective behavior is self-organizing and resilient. Because each device evaluates its state `against' its neighbors, the system can tolerate message loss and varying round frequencies by construction. The operational details remain orthogonal to the macro-program, allowing the same specification to be deployed across diverse infrastructures.

\subsection{The Layered Architecture: Theory and Tooling}

Aggregate computing organizes its research and application into a clear layered structure that separates formal theory from practical execution. At the lowest level, the formal field calculus provides the necessary mathematical rigor to reason about the correctness and self-stabilization of collective behaviors. This theoretical foundation ensures that, given a stable environment and topology, a field computation will result in a unique, stable output in finite time~\cite{DBLP:conf/saso/ViroliBDP15}.

Above the formal core, a layer of reusable libraries (such as the GCT-set) provides the engineering `bricks' needed to build complex applications. Finally, the tooling layer consists of domain-specific languages like \emph{ScaFi} (a Scala-based DSL~\cite{DBLP:journals/softx/CasadeiVAP22}) and specialized execution platforms or simulators like \emph{Alchemist}~\cite{DBLP:conf/sac/PianiniVB15}. This separation is crucial for managing the `reality gap' in \ac{CAS}; while the developer works with the clean abstractions of the calculus and libraries, the runtime and simulators handle the discrete, often unpredictable nature of real-world sensing and communication. This layered view allows aggregate computing to tackle the true issue of \ac{CAS}: by fully escaping the single `device' abstraction to program the `whole'.

\section{Simulation Landscape}
Simulation serves as a fundamental pillar in the engineering and analysis of \acp{CAS}, acting as a bridge between theoretical design and physical implementation. At its core, simulation is the process of using a mathematical or computational model to replicate the behavior of a real-world system over time~\cite{banks2005discrete}. In the context of decentralized collectives, it allows researchers to observe how local agent rules manifest into global emergent patterns within a risk-free virtual environment. By executing models that represent the logic, communication, and movement of autonomous entities, simulation provides a synthetic medium where the complexity of \acp{CAS} can be dissected and understood before any hardware is deployed.

The motivations for adopting simulation are many, primarily driven by the practical limitations of physical experimentation. First, cost represents a significant barrier; deploying hundreds or thousands of physical devices to test a self-organizing algorithm is financially and logistically prohibitive for most research cycles. Simulation mitigates this by allowing for the testing of massive populations at a fraction of the cost of hardware~\cite{DBLP:journals/isci/Schut10}. Second, time scaling provides a unique advantage, as computational models can often be executed much faster than real-time. This enables the study of long-term system evolution or the performance of millions of iterations in a matter of hours, which would otherwise take weeks or months in a real-world setting~\cite{MittalRiscoMartin2017SimBasedCAS}.

Furthermore, simulation offers levels of observability and reproducibility that are nearly impossible to achieve in the field. In a simulated environment, the state of every single agent and the properties of every communication link can be monitored and recorded without the `observer effect' interfering with the system's dynamics. This total visibility is essential for debugging the non-linear feedback loops inherent in \acp{CAS}~\cite{DBLP:journals/jos/Sargent13}. Finally, reproducibility ensures that experiments can be repeated under identical initial conditions. Since \acp{CAS} are often stochastic and sensitive to minor perturbations, the ability to control the environment and the random seeds of an experiment is vital for the statistical validation of new collective algorithms and frameworks~\cite{DBLP:journals/csur/GomesTBLV18}.

\subsection{Simulaiton techniques for \aclp{CAS}}
Simulation in the context of \aclp{CAS} is rarely a monolithic process. Because these systems involve a complex interplay of decision-making logic, network communication, environmental sensing, and physical movement, different research concerns require distinct modeling abstractions. This necessity leads to a heterogeneous simulation landscape where researchers often employ a staged approach, gradually increasing model detail as design matures~\cite{DBLP:journals/csur/GomesTBLV18}. The choice of a simulation technique typically involves a fundamental trade-off between the number of agents that can be simulated (scalability) and the accuracy of their physical representation (realism)~\cite{DBLP:journals/isci/Schut10}.

The most common starting point for exploring collective behavior is \ac{ABMS}. In this paradigm, system-level phenomena emerge from the local rules and interactions of individual agents, matching the decentralization assumptions of \acp{CAS}~\cite{DBLP:journals/csr/AbarTLO17}. \ac{ABMS} platforms are particularly valued for their ability to run large-scale parameter sweeps and explore the basic feasibility of self-organizing algorithms without the computational overhead of physical laws.

However, when \ac{CAS} components are embodied (such as in swarms of mobile robots) the physical dynamics of the agents become a critical aspect of the system's success. The way a robot moves, collides or interacts with the terrain can significantly alter emergent outcomes, making physics-based simulation essential for credible evaluation~\cite{DBLP:journals/swarm/BrambillaFBD13}. To address the realism(i.e. scalability continuum), two main types of simulators have emerged. Scalable multi-robot simulators, such as \ac{ARGoS}, target large populations by utilizing simplified physics and efficient execution loops~\cite{DBLP:journals/swarm/PinciroliTOPBBMFCDBGD12}. Conversely, higher-fidelity simulators focus on accurate contact dynamics, sophisticated perception sensors, and complex environmental interactions, though they are generally limited to smaller groups of agents due to their high computational cost~\cite{DBLP:conf/iros/KoenigH04}.

To handle systems where multiple scales or domains overlap, the literature suggests the use of \emph{co-simulation}. Co-simulation is an approach where multiple specialized simulators, each handling a different aspect like networking, control, or physics, are coordinated through an explicit coupling mechanism~\cite{DBLP:journals/csur/GomesTBLV18}. This allows heterogeneous models to interoperate, ensuring that each component of the \ac{CAS} is represented with the most appropriate level of fidelity.

Finally, to mitigate the `reality gap' (i.e. the discrepancy between simulated results and real-world performance) techniques such as \ac{HIL} and mixed reality are employed. In \ac{HIL} setups, part of the simulation is replaced by actual hardware components, allowing for the testing of real sensing and communication constraints while the rest of the environment remains virtual. These methods provide a path toward progressive realism, ensuring that self-organizing behaviors observed in a computer model remain robust when transitioned to physical reality~\cite{Sende2024}.

\subsection{Realism vs. Scalability}
The tension between realism and scalability represents a fundamental design choice in the development of \ac{CAS} simulators. Classic simulators, which emerged primarily from the social sciences and distributed computing domains, often prioritize scalability to enable the study of population-level dynamics involving millions of entities. Tools such as NetLogo, a widely adopted platform for \ac{ABMS}, exemplify this approach by utilizing a `patch-based' environment where agents move across a grid of cells~\cite{DBLP:journals/simulation/LukeCPSB05}. While NetLogo is highly effective for exploring abstract emergent phenomena and educational models, its simplified spatial representation often ignores the complex physical constraints of real-world environments. Similarly, Alchemist is a high-performance simulator specifically designed for aggregate computing and pervasive systems~\cite{DBLP:conf/sac/PianiniVB15}. It excels in scalability by abstracting environmental interactions into meta-data and focusing on the efficient execution of the field calculus across vast networks, yet it typically operates with a simplified notion of agent embodiment.

The historical focus on scalability was driven by the need to validate the statistical properties of self-organizing algorithms. However, a growing segment of the literature argues that realism must be increasingly prioritized, particularly as \acp{CAS} are deployed in safety-critical robotic and \ac{IoT} applications. Realism is not merely a matter of visual fidelity; it involves the accurate reproduction of the `physics of interaction'~\cite{DBLP:journals/swarm/BrambillaFBD13}. When a simulation overlooks the nuances of signal interference, physical collisions, or the non-linear energy consumption of hardware, it risks producing `successful' results that fail catastrophically in the field.

Focusing on realism is essential for identifying the boundary conditions of a collective's robustness. A self-healing gradient that works perfectly in a grid-based abstraction might oscillate or break when subjected to the real-world latency and packet loss characteristic of high-fidelity robotic simulators like Gazebo~\cite{DBLP:conf/iros/KoenigH04}. By acknowledging the realism/scalability trade-off, developers can choose the right tool for each stage of the engineering process: utilizing scalable tools like Alchemist for initial logic validation and high-fidelity simulators for final pre-deployment verification. This balanced approach ensures that the `whole-system' logic remains anchored in the physical realities of the individual devices.

\section{Game Engines as Simulators}

Traditionally, the simulation of complex adaptive systems has been addressed through dedicated academic tools or agent-based modeling frameworks. While these are effective for high-level abstraction, they are often limited in their physical and spatial representation. In contrast, modern game engines integrate advanced physics engines, optimized rendering pipelines, and visual development environments, enabling the creation of high-fidelity three-dimensional environments where autonomous agents operate under realistic constraints. These platforms, including Unity, Unreal Engine and Godot, provide the fundamental technologies required for real-time graphics, physics, collision detection and networking~\cite{Coronado2023XRGameEngines}.

The relevance of game engines extends significantly beyond entertainment, particularly in the fields of robotics and cyber-physical systems. They offer several advantages over traditional academic tools, such as real-time 3D rendering for qualitative analysis and integrated physics engines capable of simulating realistic dynamics. Furthermore, these engines are increasingly adopted for digital-twin workflows, where a virtual replica is synchronized with a physical asset to balance visual immersion with model accuracy~\cite{Singh2025UnityGazeboDT}. This capability is central to addressing the reality gap, as game engines support high-fidelity perception and the generation of complex environmental effects that are essential for synthetic-data generation and the transfer of learned behaviors to the real world~\cite{DBLP:conf/fsr/ShahDLK17}.

\subsection{Unity: Architecture and Core Concepts}

The project developed in this thesis is built on Unity, a game engine created by Unity Technologies that is widely utilized in both industrial and academic contexts. Unity adopts a GameObject–Component architectural model, in which every entity within a scene is represented as a container object to which modular functional components are attached. This paradigm naturally lends itself to the modeling of collective systems, as each agent can be represented as a GameObject equipped with components that implement perception, communication and decision-making logic.

Within this framework, several fundamental concepts define the simulation structure. Scenes act as hierarchical containers that organize the environment, while Prefabs allow for the creation of reusable templates for agents. The behavior of these entities is governed by lifecycle methods, such as \monospace{Update()} and \monospace{FixedUpdate()}, which ensure that time-dependent behaviors remain consistent with the engine's update cycle~\cite{unity_manual}. Physical interaction is managed by an integrated 3D physics system based on the NVIDIA PhysX SDK. This engine provides a scalable framework for modeling actors and managing both direct and proximity-based interactions, allowing for the simulation of realistic physical behaviors and dynamic effects~\cite{unity_physics}.

\subsection{Unity as a Platform for \aclp{CAS}}

The decision to adopt Unity as the platform for this study is driven by its effective balance between simulation fidelity, architectural flexibility, and development speed. Unlike higher-fidelity engines that may impose significant tooling overhead, Unity provides a streamlined workflow for agent-based modeling and accurate spatial constraints. For aggregate computing, Unity is particularly attractive as a high-fidelity environment layer; it provides the realistic geometry and sensor interfaces while the aggregate runtime handles the collective logic and neighbor interactions.

This separation of concerns allows the engine to handle embodied dynamics and scene management, while the aggregate platform manages the `computational field' through a well-defined interface. By utilizing Unity, this study leverages advanced visual debugging and automated testing workflows, ensuring that the self-organizing behaviors observed in simulation are grounded in an environment that closely approximates the complexity of physical reality.

\section{Implications for Aggregate Computing simulations}
The convergence of game engines and macro-programming models offers an architecture for the development and validation of \acp{CAS}. In this integrated landscape, game engines serve primarily as a high-fidelity environment layer, providing the realistic geometry, motion, and sensor/actuation interfaces necessary for embodied agents. Meanwhile, the aggregate programming runtime remains responsible for the core collective logic, including neighborhood interactions, the execution of asynchronous distributed rounds, and the evolution of computational fields.

This architectural separation suggests a pragmatic integration strategy that leverages the strengths of both paradigms. The engine is tasked with managing the `physical' presence of the agents—handling rigid-body dynamics, collision detection and the synthesis of raw sensor data from the virtual world. Conversely, the aggregate platform operates at a higher level of abstraction, orchestrating the global behavior of the swarm or network. The bridge between these two layers is a well-defined interface responsible for three critical functions: sensing (transforming engine-level data into field inputs), actuation (translating field outputs into engine-level movements), and neighborhood exchange (utilizing the engine’s spatial queries to determine communication topology).

By delegating environmental complexity to a game engine like Unity, researchers can focus on the resilience and correctness of the collective logic while operating in a space that closely mimics the `reality gap' found in physical deployments. This approach moves beyond the simplified grid-based or meta-data abstractions of classic simulators, providing a robust pathway for the engineering of situated systems where physical constraints can measurably impact the emergent outcome. The feasibility and limitations of this coupled approach provide the direct motivation for the integration of an aggregate programming implementation within the Unity ecosystem, as explored in the following chapters of this study.
