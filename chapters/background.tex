\chapter{Background and State of the Art}

To contextualize the contributions of this thesis, it is necessary to establish the theoretical foundations upon which it is built. This chapter explores the evolution of distributed systems toward collective intelligence and examines the formalisms of self-organizing frameworks. By evaluating the limitations of current simulators, this chapter identifies the technical `reality gap' that this research aims to bridge, providing the necessary background to appreciate the integration of high-fidelity game engines into the decentralized coordination workflow.

\section{Distributed Systems and Organizational Complexity}

The foundational definition of a distributed system is provided by Andrew S. Tanenbaum and Maarten Van Steen, who describe it as ``a collection of autonomous computing elements that appears to its users as a single coherent system''~\cite{Tanenbaum_Van_Steen_2001b}. This definition establishes a fundamental tension between the independent nature of the underlying hardware and the unified experience required by the end-user. To fully understand the implications of this paradigm, it is necessary to examine these two components individually.

\paragraph{Collection of autonomous computing elements}
The first part of the definition emphasizes the heterogeneity and autonomy of the system's components. These nodes can range from high-performance server clusters to resource-constrained microcontrollers, each operating independently without a shared global clock. Consequently, synchronization becomes a primary challenge, as nodes must rely on message-passing to coordinate their actions toward a common goal.

Furthermore, the nature of these collections is increasingly moving from static, hard-wired topologies toward dynamic environments where nodes join and leave the system, a phenomenon known as \emph{churn}~\cite{stutzbach2006understanding}. In such dynamic settings, maintaining a reliable communication path between any two nodes is not guaranteed and must be managed through robust routing and discovery protocols. This autonomy also introduces security and confidentiality concerns: as nodes act independently, the overhead of authentication and secure message exchange can often become a bottleneck for system-wide scalability.

\paragraph{Single coherent system}
Despite the underlying decentralization, the system must present itself as a unified entity. Coherence implies that the system's behavior remains consistent and predictable from the user's perspective, regardless of where or when the interaction occurs. This requirement leads to the concept of \emph{distribution transparency}, which aims to hide the complexities of the distributed nature of the system from both users and application developers.

A critical aspect of achieving this coherence is the management of failures. In a distributed context, the `perfect network' is a fallacy; instead, systems must be designed under the assumption that components will fail. Treating failures as first-class citizens ensures that the system can maintain its coherent state through fault-tolerance mechanisms, preventing local disruptions from cascading into global system outages.

\paragraph{The Role of Middleware}
To bridge the gap between autonomous nodes and the need for coherence, a \emph{middleware} layer is typically introduced~\cite{Bernstein_1996}. Middleware acts as an abstraction layer that sits between the operating system and the distributed application, providing a standardized interface for resource sharing and communication. By masking the differences in hardware and local software, middleware enables the development of complex distributed logic without requiring the programmer to manage low-level networking details. In the context of this research, \acp{CAS} and \acp{AC} frameworks can be viewed as specialized middleware designed to manage collective intelligence across high-density, pervasive networks.

\subsection{\aclp{DS} Goals} \label{subsec:ds-goals}

sharing res, transparency, openness, scalability

\subsubsection{Distribution transparency}

access, location, relocation, migration, replication, concurrency, failure

\subsection{\aclp{DS} Kinds}

\paragraph{High-Performance Distributed Systems}

\paragraph{Distributed Information Systems}

\paragraph{Pervasive Systems}

\section{Self-Organizing Frameworks}
\begin{itemize}
  \item use mirko slides
  \item CAS charateristics
  \item CAS behavior description (describe the whole, not the single)
  \item possible solutions to CAS representation (macro-programming, find other ways)
\end{itemize}
\subsection{Aggregate Computing}
\begin{itemize}
  \item use mirko slides
  \item field calculus
  \item the program of the single (read local sensors and messages received, compute the shared aggregate program against the previous state (if any), produce a new state and a message to be sent to neighbors, send message to its neighbors)
  \item split view between theory (continuous, correctness, composability) and tooling (discrete, reality)
\end{itemize}
\section{Simulation Landscape}
\begin{itemize}
  \item what is simulation
  \item motivations: costs, time scaling, observability, reproducibility
\end{itemize}

\subsection{Simulaiton techniques for \aclp{CAS}}
\begin{itemize}
  \item not monolithic because different concerns to test requires different modeling
  \item ABMS
  \item CAS components embodyment(?) --- physics dynamics becomes a critical aspect
  \item currently it exists scalable multi-robot simulators - large population, simplified physics
  \item higher-fidelity simulators - accurate contact dynamics, perception, env interaction
  \item co-simulators (Idk what is it)
  \item HIL and mixed reality
\end{itemize}
\subsection{The Reality Gap}
\begin{itemize}
  \item moving from simulation to reality
  \item increasingly detailed simulation
  \item hardware-in-the-loop (part of the simulation substituted by real hardware)
  \item mixed-reality
\end{itemize}
\subsection{Realism vs. Scalability}

\begin{itemize}
  \item classic simulators focuses on scalability
  \item alchemist(AC) and NetLogo(ABMS)
  \item why realism should be focused
\end{itemize}

\section{Game Engines as Simulators}

\begin{itemize}
  \item used particularly in robotics-oriented simulation
  \item used for digital twins
  \item can generate visually rich and complex environment effects
  \item what's below
\end{itemize}

Traditionally, the simulation of complex systems has been addressed through dedicated academic tools or agent-based modeling frameworks, often characterized by a high level of abstraction, but limited in their physical and spatial representation. In contrast, modern game engines integrate advanced physics engines, optimized rendering pipelines, scripting tools, and visual development environments, enabling the creation of high-fidelity three-dimensional environments in which autonomous agents can operate under realistic constraints.

\subsection{What is a Game Engine}

A game engine is the core software of a video game or any other application that uses real-time graphics. It provides the fundamental technologies, simplifies the development process, and often allows the game to run on different platforms, such as consoles and computer operating systems. The main features typically offered by a game engine include a rendering system for 2D and 3D graphics, physics and collision detection, audio management, scripting, animations, artificial intelligence, networking and scene-graph management.

Game engines often provide a suite of visual development tools in addition to reusable software components. These tools are generally provided in an integrated development environment to enable a simplified and rapid development of games in a data-driven manner~\cite{wiki_gameengine}.

At present, Unity, Unreal Engine and Godot are the three most widely used options among the game development community.

\subsection{Relevance of Game Engines Beyond Gaming}

Adopting a game engine as a simulation platform offers several advantages over traditional academic tools.

Provides real-time 3D rendering, which supports both qualitative analysis and debugging processes.
It has integrated physics engines capable of simulating realistic dynamics.
The visual development environment enables rapid prototyping and iterative design.
Most popular game engines have a mature ecosystem of tools and plugins and also support for automated execution and batch processing.

\subsection{Unity: Core Concepts and Functionalities}

The project developed in this thesis is built on Unity, a game engine created by Unity Technologies and widely used in both industrial and academic contexts fig.\ref{fig:unity-editor}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/unity-editor.png}
    \caption{The main view of the Unity Editor~\cite{unity_projectview}.}
    \label{fig:unity-editor}
\end{figure}

Unity adopts a \emph{GameObject–Component} architectural model, in which every entity within a scene is represented as a container object to which modular components are attached. Behaviors are implemented through C\# scripts that interact with the engine’s update cycle.

The fundamental concepts include:
\begin{itemize}
    \item \textbf{Scenes}: hierarchical object containers that define and organize the simulated environment.
    \item \textbf{GameObject}: the basic entity of the simulation.
    \item \textbf{Component}: functional modules that can be applyed to GameObjects (e.g., Transform, Rigidbody, Collider).
    \item \textbf{Prefab}: reusable templates.
    \item \textbf{Lifecycle methods}: methods such as \monospace{Update()} and \monospace{FixedUpdate()} that define time-dependent behavior~\cite{unity_manual}.
\end{itemize}

This paradigm naturally lends itself to the modeling of collective systems: each agent can be represented as a GameObject equipped with components that implement perception, communication, and decision-making behavior.

Within this architectural framework, physical interaction and spatial dynamics are handled by Unity’s built-in 3D physics system, which is based on the integration of NVIDIA’s PhysX engine, developed in close collaboration with NVIDIA.

The NVIDIA PhysX SDK is an open-source, scalable real-time physics engine designed to support advanced simulations, enabling more immersive gameplay through realistic physical behavior and dynamic real-time effects. It provides a framework for modeling 3D environments, allowing developers to create and remove physical entities (actors) and manage both direct and proximity-based interactions between them \cite{unity_physics}.

\subsection{Unity as the Preferred Platform for This Study}

The decision to adopt Unity as the platform for simulating the collective system explored in this thesis is driven by several considerations.

Unity provides the capability for realistic three-dimensional simulation and integrates naturally with an agent-based modeling approach. It allows for the accurate representation of physical and spatial constraints, offers advanced visual debugging features, and supports automation and testing workflows, making it a highly suitable environment for this study.

A comparison with Unreal Engine shows that, although the latter offers a more advanced graphics pipeline, Unity represents an effective balance between simulation fidelity, architectural flexibility, and development speed.

\section{implications for aggregate-computing simulations}
\begin{itemize}
  \item exploited as high fidelity env
  \item game engine responsible for sensors, actuators and neighborhood exchange
  \item CAS responsible for neighbor interaction, distributed rounds and collective logic
\end{itemize}
