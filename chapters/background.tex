\chapter{Background and State of the Art}

To contextualize the contributions of this thesis, it is necessary to establish the theoretical foundations upon which it is built. This chapter explores the evolution of distributed systems toward collective intelligence and examines the formalisms of self-organizing frameworks. By evaluating the limitations of current simulators, this chapter identifies the technical `reality gap' that this research aims to bridge, providing the necessary background to appreciate the integration of high-fidelity game engines into the decentralized coordination workflow.

\section{Distributed Systems}

The foundational definition of a distributed system is provided by Andrew S. Tanenbaum and Maarten Van Steen, who describe it as ``a collection of autonomous computing elements that appears to its users as a single coherent system''~\cite{Tanenbaum_Van_Steen_2001b}. This definition establishes a fundamental tension between the independent nature of the underlying hardware and the unified experience required by the end-user. To fully understand the implications of this paradigm, it is necessary to examine these two components individually.

\paragraph{Collection of autonomous computing elements}
The first part of the definition emphasizes the heterogeneity and autonomy of the system's components. These nodes can range from high-performance server clusters to resource-constrained microcontrollers, each operating independently without a shared global clock. Consequently, synchronization becomes a primary challenge, as nodes must rely on message-passing to coordinate their actions toward a common goal.

Furthermore, the nature of these collections is increasingly moving from static, hard-wired topologies toward dynamic environments where nodes join and leave the system, a phenomenon known as \emph{churn}~\cite{stutzbach2006understanding}. In such dynamic settings, maintaining a reliable communication path between any two nodes is not guaranteed and must be managed through robust routing and discovery protocols. This autonomy also introduces security and confidentiality concerns: as nodes act independently, the overhead of authentication and secure message exchange can often become a bottleneck for system-wide scalability.

\paragraph{Single coherent system}
Despite the underlying decentralization, the system must present itself as a unified entity. Coherence implies that the system's behavior remains consistent and predictable from the user's perspective, regardless of where or when the interaction occurs. This requirement leads to the concept of \emph{distribution transparency}, which aims to hide the complexities of the distributed nature of the system from both users and application developers.

A critical aspect of achieving this coherence is the management of failures. In a distributed context, the `perfect network' is a fallacy; instead, systems must be designed under the assumption that components will fail. Treating failures as first-class citizens ensures that the system can maintain its coherent state through fault-tolerance mechanisms, preventing local disruptions from cascading into global system outages.

\paragraph{The Role of Middleware}
To bridge the gap between autonomous nodes and the need for coherence, a \emph{middleware} layer is typically introduced~\cite{Bernstein_1996}. Middleware acts as an abstraction layer that sits between the operating system and the distributed application, providing a standardized interface for resource sharing and communication. By masking the differences in hardware and local software, middleware enables the development of complex distributed logic without requiring the programmer to manage low-level networking details. In the context of this research, \acp{CAS} and \acp{AC} frameworks can be viewed as specialized middleware designed to manage collective intelligence across high-density, pervasive networks.

\subsection{\aclp{DS} Goals}

As cited in~\cref{chap:intro}, the necessity of \ac{DS} arose from the need of coordinating complex, geographically distributed nodes. In particular, the main goals of a \ac{DS} are
%
\begin{inlinelist}
  \item supporting sharing of resources across the network;
  \item hiding the network itself from the user;
  \item be open;
  \item easy scaling.
\end{inlinelist}
%
Building a \ac{DS} is not an easy task therefore these goals should be met in order to making it worth the effort.

\paragraph{Sharing Resources}
The primary motivation for building a distributed system is to facilitate the seamless sharing of resources among a collection of users and applications. In this context, a `resource' is defined broadly, encompassing everything from hardware components like high-capacity storage and specialized sensors to software-defined entities such as data sets and services. From an economic perspective, this allows for the creation of more cost-effective systems by leveraging remote, centralized assets thereby reducing the need for expensive local infrastructure.

Beyond cost efficiency, resource sharing is the fundamental enabler of collaboration in decentralized environments. By allowing autonomous nodes to exchange information and access shared state, the system can tackle complex tasks that would be impossible for a single isolated node to perform.

\paragraph{Distribution Transaprency}
The concept of information hiding is a cornerstone of modern software engineering, allowing developers and users to operate at higher levels of abstraction while delegating low-level complexities to the underlying machinery. In the context of distributed systems, this is realized through \emph{distribution transparency}, which aims to present the collection of autonomous nodes as a single, coherent entity. This transparency is not a monolithic feature but a multi-dimensional goal comprising several distinct types.

\emph{Access} transparency ensures that the way data is represented remains decoupled from the method used to access it, allowing heterogeneous machines to exchange information despite differing internal architectures. \emph{Location} transparency masks the physical coordinates of a resource, much like a \ac{URL} provides a logical address without revealing the specific geographic or network location of a server. Related to this are \emph{migration} and \emph{relocation} transparency, which hide the movement of resources; the former applies to resources moved while not in use, while the latter ensures that a resource can be moved during active use without interrupting the user's operation.

To ensure performance and reliability, \emph{replication} transparency hides the fact that multiple copies of data may exist across the network, while \emph{concurrency} transparency allows multiple users to share a single resource simultaneously without realizing that others are accessing the same data. Finally, \emph{failure} transparency is perhaps the most critical for system resilience, as it attempts to mask the malfunction and subsequent recovery of components, ensuring that the user remains unaware of any underlying faults.

However, achieving full transparency is often more of a strategic trade-off than an absolute requirement. A system that over-prioritizes transparency may inadvertently degrade the user experience. For instance, an aggressive pursuit of failure transparency might lead to a system that hangs indefinitely while repeatedly retrying a connection to a dead server, rather than promptly informing the user of the failure.

\paragraph{Openness}

An open distributed system is characterized by its ability to be easily extended and integrated with other heterogeneous components. According to Tanenbaum and Van Steen, this goal is achieved by ensuring that components adhere to standardized rules governing their syntax and semantics. The practical value of openness is measured through two key metrics: interoperability and portability. Interoperability refers to the extent to which components from different manufacturers can work together based on a common standard, while portability describes the ease with which an application can be moved from one distributed system to another without modification. Furthermore, an open system must be extensible, allowing developers to add or replace internal parts without disrupting the existing infrastructure. This requires moving away from monolithic architectures toward a modular design where internal interfaces are as well-defined as external ones.

A critical design principle for achieving this flexibility is the separation of policy from mechanism. This paradigm suggests that a system should provide a set of general-purpose mechanisms (e.g. the ability to cache data or route messages) while leaving the specific policies (e.g. which data to cache or how long to store it) to the user or application developer. While a strict separation can lead to complex configuration problems, it is essential for creating adaptable systems. In the context of this research, this principle is foundational: the game engine provides the physical and spatial mechanisms, while the Aggregate Computing framework implements the coordination policies that govern the collective's behavior. This balance allows the system to remain open to new environmental constraints or logic changes without requiring a complete redesign of the underlying simulation or communication layers.

\paragraph{Scalability}
Scalability is the ability of a system to handle increasing load without a proportional loss in performance, and it is measured along three dimensions: size, geography, and administration. Size scalability addresses the elimination of centralized bottlenecks (e.g. CPU or bandwidth limits) that occur when a single server is overwhelmed by requests. Geographical scalability deals with the transition from local-area networks to wide-area systems, where synchronous communication becomes impractical due to significant signal latency. Finally, administrative scalability ensures the system remains manageable across multiple trust domains and conflicting security policies.

To achieve these goals, systems typically rely on three core techniques: hiding communication latencies, distribution, and replication. By moving toward asynchronous communication, the system can mask the delays inherent in distant or unreliable links. Furthermore, by distributing work and replicating data across the network, the system avoids single points of failure and bottlenecking.

\subsection{\aclp{DS} Kinds}

To conclude the foundational overview of distributed systems, it is essential to categorize them based on their application focus. While the underlying principles of resource sharing and transparency remain constant, the architectural priorities shift significantly depending on whether the system is designed for raw computation, data management, or environmental integration.

\paragraph{High-Performance Distributed Systems}
Historically, the need for massive computational power led to the development of systems focused on parallel processing. This category includes cluster computing, where a collection of similar workstations is connected via a high-speed local area network to act as a single multi-processor (such as the distributed shared-memory multicomputers~\cite{Amar_Barak_Shiloh_2004}), and grid computing, which federates resources from diverse administrative domains to solve large-scale scientific problems (such as~\cite{Foster_Kesselman_Tuecke_2004}). More recently, cloud computing has evolved these concepts into a utility model, providing scalable, on-demand virtualized resources. In these systems, the primary metric of success is throughput and the efficient distribution of heavy workloads across a stable, high-performance infrastructure.

\paragraph{Distributed Information Systems}
As businesses grew, the challenge shifted from raw calculation to the management of vast amounts of networked data. These systems often evolved from the need to integrate legacy applications into a coherent workflow. Distributed transaction processing ensures that complex operations across multiple databases (such as a bank transfer) remain atomic and consistent, while Enterprise Application Integration~\cite{Enterprise} focuses on allowing decoupled applications to communicate via middleware. Here, the focus is on data integrity and the seamless interoperability of heterogeneous software components.

\paragraph{Pervasive Systems}
The most recent evolution, and the one most pertinent to this research, is the rise of pervasive (or ubiquitous) systems. Unlike the previous categories, which are often composed of stable, `tethered' nodes, pervasive systems are characterized by their integration into the physical environment. These systems are typically composed of small, mobile, and battery-powered devices that must operate with high degrees of instability.

In pervasive environments, the traditional goals of distribution transparency must be re-evaluated. Because these nodes are often context-aware and physically embodied, their `location' is not a detail to be hidden, but a primary input for their logic. Furthermore, the inherent volatility of these networks, where nodes frequently join, leave or fail due to energy constraints, makes centralized management impossible. This shift from static, managed clusters to fluid, unmanaged environments provides the natural transition toward Self-Organizing Frameworks. To maintain a `single coherent system' view in such chaotic settings, we must move beyond simple middleware toward collective adaptive strategies that allow global behavior to emerge from local interactions.

\section{\aclp{CAS}}

\aclp{CAS} represent a sophisticated class of decentralized architectures where global behavior emerges from the nonlinear interactions of numerous autonomous agents~\cite{holland1992adaptation}. Unlike traditional distributed systems that often rely on structured hierarchies or strict consensus protocols, a \ac{CAS} is defined by a large number of spatially situated devices that operate under a set of fundamental constraints. These include predominantly local interactions and a high degree of dynamism. This dynamism manifests as constant fluctuations in the environment and the network topology, often driven by agent mobility, hardware failures, or intermittent connectivity~\cite{DBLP:journals/csur/Casadei23}. Furthermore, these systems typically function in an open setting, an environment where components may appear or disappear over time without destabilizing the system's core utility~\cite{DBLP:journals/jlap/ViroliBDACP19}.

A great challenge in the study of \ac{CAS} is the shift in perspective required to describe behavior. While traditional software engineering focuses on the `single', the desired outcomes in a \ac{CAS} are naturally expressed at the `whole' or collective level. Examples of such outcomes include the estimation of a spatial distribution, the formation of a self-healing geometric pattern, or the adaptive routing of information through a shifting region of space. The core difficulty lies in the gap between this global intent and the local execution; because the system is sensitive to its environment, minor changes in topology, device density, or message timing can lead to qualitatively different emergent outcomes~\cite{BealIEEEComputer2015}. Consequently, the behavior of a \ac{CAS} cannot be predicted by simply summing the individual parts, as the recursive feedback loops between agents create a system-wide complexity that transcends the logic of any single device.

\subsection{Self-Organizing Frameworks}

The engineering of self-organizing systems has led to a variety of frameworks aimed at narrowing the gap between local rules and global emergence. These frameworks generally fall into three categories: bio-inspired coordination models, virtual physics-based approaches, and general-purpose macro-programming languages. Each provides a different lens through which to represent and control the collective state of a \ac{CAS}.

Bio-inspired frameworks often take cues from natural phenomena such as ant colony behavior or chemical gradients. One notable example is the use of \emph{stigmergy}, where agents interact indirectly by modifying their environment (for instance, through digital pheromones that guide other agents toward specific tasks or regions~\cite{DBLP:journals/computer/GardelliVZ08}). Similarly, chemical-inspired models like the \ac{CHAM} treat the system as a solution where floating molecules (data) interact according to fixed reaction rules, allowing global patterns to emerge from purely local, stochastic encounters~\cite{DBLP:journals/tcs/BerryB92}. These models are inherently robust to the arrival and departure of components, as the coordination logic is embedded in the environment rather than in a fixed network map.

Virtual physics-based approaches, such as \ac{APF}, represent the collective by mapping agent interactions to physical forces. In this representation, goals exert attractive forces while obstacles or neighbors exert repulsive forces. The resulting behavior is a continuous movement toward a state of minimum energy, which naturally translates to tasks like collision avoidance or flocking~\cite{DBLP:journals/ijrr/Khatib86}. While highly effective for spatial coordination, these models can struggle with complex logical tasks that require discrete state transitions or symbolic reasoning across the entire network.

A more recent shift toward addressing these limitations is found in formal macro-programming languages. For instance, the \ac{XC} provides a functional foundation where state and interaction are unified into a single primitive, allowing for a more general representation of collective processes that can spread, shrink, or move across a network~\cite{DBLP:journals/fgcs/CasadeiVAP24}. These frameworks move away from specific metaphors (like physics or biology) toward a more general-purpose computational logic.

Among these solutions, \emph{aggregate programming} has emerged as a particularly comprehensive framework for spatially situated collectives. It leverages the abstraction of the \emph{computational field} to represent system-wide data as a continuous entity across space and time. By focusing on field transformations rather than individual message passing, it provides a mathematical and operational bridge to handle the inherent complexity of \acp{CAS}. This approach forms the basis for the subsequent discussion on aggregate computing and its role in modern distributed systems engineering.

\subsection{Aggregate Computing}
\begin{itemize}
  \item use mirko slides
  \item field calculus
  \item the program of the single (read local sensors and messages received, compute the shared aggregate program against the previous state (if any), produce a new state and a message to be sent to neighbors, send message to its neighbors)
  \item split view between theory (continuous, correctness, composability) and tooling (discrete, reality)
\end{itemize}
\section{Simulation Landscape}
\begin{itemize}
  \item what is simulation
  \item motivations: costs, time scaling, observability, reproducibility
\end{itemize}

\subsection{Simulaiton techniques for \aclp{CAS}}
\begin{itemize}
  \item not monolithic because different concerns to test requires different modeling
  \item ABMS
  \item CAS components embodyment(?) --- physics dynamics becomes a critical aspect
  \item currently it exists scalable multi-robot simulators - large population, simplified physics
  \item higher-fidelity simulators - accurate contact dynamics, perception, env interaction
  \item co-simulators (Idk what is it)
  \item HIL and mixed reality
\end{itemize}
\subsection{The Reality Gap}
\begin{itemize}
  \item moving from simulation to reality
  \item increasingly detailed simulation
  \item hardware-in-the-loop (part of the simulation substituted by real hardware)
  \item mixed-reality
\end{itemize}
\subsection{Realism vs. Scalability}

\begin{itemize}
  \item classic simulators focuses on scalability
  \item alchemist(AC) and NetLogo(ABMS)
  \item why realism should be focused
\end{itemize}

\section{Game Engines as Simulators}

\begin{itemize}
  \item used particularly in robotics-oriented simulation
  \item used for digital twins
  \item can generate visually rich and complex environment effects
  \item what's below
\end{itemize}

Traditionally, the simulation of complex systems has been addressed through dedicated academic tools or agent-based modeling frameworks, often characterized by a high level of abstraction, but limited in their physical and spatial representation. In contrast, modern game engines integrate advanced physics engines, optimized rendering pipelines, scripting tools, and visual development environments, enabling the creation of high-fidelity three-dimensional environments in which autonomous agents can operate under realistic constraints.

\subsection{What is a Game Engine}

A game engine is the core software of a video game or any other application that uses real-time graphics. It provides the fundamental technologies, simplifies the development process, and often allows the game to run on different platforms, such as consoles and computer operating systems. The main features typically offered by a game engine include a rendering system for 2D and 3D graphics, physics and collision detection, audio management, scripting, animations, artificial intelligence, networking and scene-graph management.

Game engines often provide a suite of visual development tools in addition to reusable software components. These tools are generally provided in an integrated development environment to enable a simplified and rapid development of games in a data-driven manner~\cite{wiki_gameengine}.

At present, Unity, Unreal Engine and Godot are the three most widely used options among the game development community.

\subsection{Relevance of Game Engines Beyond Gaming}

Adopting a game engine as a simulation platform offers several advantages over traditional academic tools.

Provides real-time 3D rendering, which supports both qualitative analysis and debugging processes.
It has integrated physics engines capable of simulating realistic dynamics.
The visual development environment enables rapid prototyping and iterative design.
Most popular game engines have a mature ecosystem of tools and plugins and also support for automated execution and batch processing.

\subsection{Unity: Core Concepts and Functionalities}

The project developed in this thesis is built on Unity, a game engine created by Unity Technologies and widely used in both industrial and academic contexts fig.\ref{fig:unity-editor}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/unity-editor.png}
    \caption{The main view of the Unity Editor~\cite{unity_projectview}.}
    \label{fig:unity-editor}
\end{figure}

Unity adopts a \emph{GameObject–Component} architectural model, in which every entity within a scene is represented as a container object to which modular components are attached. Behaviors are implemented through C\# scripts that interact with the engine’s update cycle.

The fundamental concepts include:
\begin{itemize}
    \item \textbf{Scenes}: hierarchical object containers that define and organize the simulated environment.
    \item \textbf{GameObject}: the basic entity of the simulation.
    \item \textbf{Component}: functional modules that can be applyed to GameObjects (e.g., Transform, Rigidbody, Collider).
    \item \textbf{Prefab}: reusable templates.
    \item \textbf{Lifecycle methods}: methods such as \monospace{Update()} and \monospace{FixedUpdate()} that define time-dependent behavior~\cite{unity_manual}.
\end{itemize}

This paradigm naturally lends itself to the modeling of collective systems: each agent can be represented as a GameObject equipped with components that implement perception, communication, and decision-making behavior.

Within this architectural framework, physical interaction and spatial dynamics are handled by Unity’s built-in 3D physics system, which is based on the integration of NVIDIA’s PhysX engine, developed in close collaboration with NVIDIA.

The NVIDIA PhysX SDK is an open-source, scalable real-time physics engine designed to support advanced simulations, enabling more immersive gameplay through realistic physical behavior and dynamic real-time effects. It provides a framework for modeling 3D environments, allowing developers to create and remove physical entities (actors) and manage both direct and proximity-based interactions between them \cite{unity_physics}.

\subsection{Unity as the Preferred Platform for This Study}

The decision to adopt Unity as the platform for simulating the collective system explored in this thesis is driven by several considerations.

Unity provides the capability for realistic three-dimensional simulation and integrates naturally with an agent-based modeling approach. It allows for the accurate representation of physical and spatial constraints, offers advanced visual debugging features, and supports automation and testing workflows, making it a highly suitable environment for this study.

A comparison with Unreal Engine shows that, although the latter offers a more advanced graphics pipeline, Unity represents an effective balance between simulation fidelity, architectural flexibility, and development speed.

\section{implications for aggregate-computing simulations}
\begin{itemize}
  \item exploited as high fidelity env
  \item game engine responsible for sensors, actuators and neighborhood exchange
  \item CAS responsible for neighbor interaction, distributed rounds and collective logic
\end{itemize}
